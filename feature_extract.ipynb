{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a57aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bids import BIDSLayout\n",
    "import os\n",
    "\n",
    "# Define your root directory\n",
    "data_path = '/Volumes/T9/ds001486/derivatives/fmriprep'\n",
    "\n",
    "# Initialize the layout\n",
    "layout = BIDSLayout(data_path, validate=False, derivatives=False)\n",
    "\n",
    "# Get all preprocessed BOLD files for the math task\n",
    "# This returns a list of objects containing path info\n",
    "bold_files = layout.get(suffix='bold', \n",
    "                        extension='nii.gz', \n",
    "                        desc='preproc', \n",
    "                        return_type='file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c019cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from nilearn.maskers import NiftiLabelsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn import datasets\n",
    "\n",
    "# --- 1. Your Defined Groups ---\n",
    "mld_subs = ['059', '065', '067', '069', '071', '075', '076', '077', \n",
    "            '078', '083', '088', '095', '096', '103', '106']\n",
    "\n",
    "td_subs = ['090', '036', '013', '008', '057', '070', '023', '024', \n",
    "           '053', '044', '034', '060', '007', '027', '010']\n",
    "\n",
    "# --- 2. Setup Atlas and Masker ---\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, yeo_networks=7)\n",
    "\n",
    "masker = NiftiLabelsMasker(labels_img=atlas.maps, standardize=True, memory=None)\n",
    "conn_measure = ConnectivityMeasure(kind='correlation', vectorize=True, discard_diagonal=True)\n",
    "\n",
    "labels = atlas.labels \n",
    "\n",
    "# Since labels are often 'bytes' in Python, let's clean them to strings\n",
    "labels = [label.decode('utf-8') if isinstance(label, bytes) else label for label in labels]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "X = []          \n",
    "y_group = []    \n",
    "y_task = []     # 1 for Mult, 0 for Sub\n",
    "groups = []     \n",
    "\n",
    "print(\"Starting feature extraction (Math only)...\")\n",
    "\n",
    "# Output folder on your Desktop to avoid read-only errors\n",
    "output_dir = os.path.expanduser('~/Desktop/math_results')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for bold_path in bold_files:\n",
    "    filename = os.path.basename(bold_path)\n",
    "    \n",
    "    # 1. Precise Task Filtering\n",
    "    # We explicitly check for Mult and Sub to avoid labeling Rhyming/Num as Sub\n",
    "    if 'task-Mult' in filename:\n",
    "        current_task = 1\n",
    "    elif 'task-Sub' in filename:\n",
    "        current_task = 0\n",
    "    else:\n",
    "        # Skips Rhyming, Num, or any other non-math tasks\n",
    "        continue \n",
    "\n",
    "    # 2. Confound Path Construction\n",
    "    parts = filename.split('_')\n",
    "    essential_parts = [p for p in parts if any(x in p for x in ['sub-', 'ses-', 'task-', 'run-'])]\n",
    "    confound_name = \"_\".join(essential_parts) + \"_desc-confounds_timeseries.tsv\"\n",
    "    confound_path = os.path.join(os.path.dirname(bold_path), confound_name)\n",
    "\n",
    "    if not os.path.exists(confound_path):\n",
    "        continue\n",
    "\n",
    "    # 3. Subject ID and Grouping (MLD vs TD)\n",
    "    sub_id = filename.split('_')[0].split('-')[1] \n",
    "    group_label = 1 if sub_id in mld_subs else 0 #\n",
    "    \n",
    "    try:\n",
    "        # Load and clean confounds (Handles NaNs and non-numeric columns)\n",
    "        df = pd.read_csv(confound_path, sep='\\t')\n",
    "        df_numeric = df.select_dtypes(include=[np.number])\n",
    "        df_clean = df_numeric.fillna(0).dropna(axis=1, how='all')\n",
    "        \n",
    "        # Extraction with standardized sample scaling\n",
    "        masker.set_params(standardize='zscore_sample')\n",
    "        conn_measure.set_params(standardize='zscore_sample') \n",
    "        \n",
    "        time_series = masker.fit_transform(bold_path, confounds=df_clean)\n",
    "        \n",
    "        # Quality Check for Signal\n",
    "        if np.any(np.isnan(time_series)) or np.any(np.isinf(time_series)):\n",
    "            print(f\"{sub_id} has bad signal (NaNs/Infs). Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        correlation_vector = conn_measure.fit_transform([time_series])[0]\n",
    "        \n",
    "        # --- SUCCESS ---\n",
    "        X.append(correlation_vector)\n",
    "        y_group.append(group_label)\n",
    "        y_task.append(current_task) # Store 1 for Mult, 0 for Sub\n",
    "        groups.append(sub_id) \n",
    "        task_name = \"Mult\" if current_task == 1 else \"Sub\"\n",
    "        print(f\"{sub_id} | {task_name} added successfully.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on {sub_id}: {e}\")\n",
    "\n",
    "# --- FINAL SAVE ---\n",
    "# Saving all 4 arrays to your Desktop\n",
    "X = np.array(X)\n",
    "y = np.array(y_group)\n",
    "yt = np.array(y_task)\n",
    "g = np.array(groups)\n",
    "\n",
    "np.save(os.path.join(output_dir, 'X_features.npy'), X)\n",
    "np.save(os.path.join(output_dir, 'y_labels.npy'), y)\n",
    "np.save(os.path.join(output_dir, 'y_tasks.npy'), yt) # New file for task labels\n",
    "np.save(os.path.join(output_dir, 'subject_groups.npy'), g)\n",
    "\n",
    "print(f\"\\nDONE! Saved {len(y)} samples to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmriprep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
